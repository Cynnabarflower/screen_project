# -*- coding: utf-8 -*-
"""ÐšÐ¾Ð¿Ð¸Ñ Ð±Ð»Ð¾ÐºÐ½Ð¾Ñ‚Ð° "Binary segmentation intro"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HJHIVWCQagRIHQ1GWanQ4dnupV911GNg

# ðŸ‡­ ðŸ‡ª ðŸ‡± ðŸ‡± ðŸ‡´ ðŸ‘‹

This example shows how to use `segmentation-models-pytorch` for **binary** semantic segmentation. We will use the [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) (this is an adopted example from Albumentations package [docs](https://albumentations.ai/docs/examples/pytorch_semantic_segmentation/), which is strongly recommended to read, especially if you never used this package for augmentations before). 

The task will be to classify each pixel of an input image either as pet ðŸ¶ðŸ± or as a background.


What we are going to overview in this example:  

 - ðŸ“œ `Datasets` and `DataLoaders` preparation (with predefined dataset class).  
 - ðŸ“¦ `LightningModule` preparation: defining training, validation and test routines.  
 - ðŸ“ˆ Writing `IoU` metric inside the `LightningModule` for measuring quality of segmentation.  
 - ðŸ¶ Results visualization.


> It is expected you are familiar with Python, PyTorch and have some experience with training neural networks before!
"""

IMAGE_WIDTH = 256
IMAGE_HEIGHT = 256

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/gdrive/', force_remount=True)
# !mkdir /content/gdrive/MyDrive/screen_project
# %cd /content/gdrive/MyDrive/screen_project

# !ls
#
# !unzip InputData512Linear.zip -d input_data

from PIL import Image

from pathlib import Path

# !pip install segmentation-models-pytorch
# !pip install "pytorch-lightning==1.9.5"

# !pip install git+https://github.com/PyTorchLightning/pytorch-lightning
if __name__ == '__main__':
    import os
    import torch
    import matplotlib.pyplot as plt
    import pytorch_lightning as pl
    import segmentation_models_pytorch as smp

    from pprint import pprint
    from torch.utils.data import DataLoader

    """## Dataset
    
    In this example we will use predefined `Dataset` class for simplicity. The dataset actually read pairs of images and masks from disk and return `sample` - dictionary with keys `image`, `mask` and others (not relevant for this example).
    
    âš ï¸ **Dataset preparation checklist** âš ï¸
    
    In case you writing your own dataset, please, make sure that:
    
    1.   **Images** ðŸ–¼  
        âœ…   Images from dataset have **the same size**, required for packing images to a batch.  
        âœ…   Images height and width are **divisible by 32**. This step is important for segmentation, because almost all models have skip-connections between encoder and decoder and all encoders have 5 downsampling stages (2 ^ 5 = 32). Very likely you will face with error when model will try to concatenate encoder and decoder features if height or width is not divisible by 32.  
        âœ…   Images have **correct axes order**. PyTorch works with CHW order, we read images in HWC [height, width, channels], don`t forget to transpose image.
    2.   **Masks** ðŸ”³  
        âœ…   Masks have **the same sizes** as images.   
        âœ…   Masks have only `0` - background and `1` - target class values (for binary segmentation).  
        âœ…   Even if mask don`t have channels, you need it. Convert each mask from **HW to 1HW** format for binary segmentation (expand the first dimension).
    
    Some of these checks are included in LightningModule below during the training.
    
    â—ï¸ And the main rule: your train, validation and test sets are not intersects with each other!
    """

    from segmentation_models_pytorch.datasets import SimpleOxfordPetDataset

    # download data
    root = "."
    # SimpleOxfordPetDataset.download(root)

    import os
    import torch
    import shutil
    import numpy as np

    from PIL import Image
    from tqdm import tqdm
    from urllib.request import urlretrieve


    class OxfordPetDataset(torch.utils.data.Dataset):
        def __init__(self, root, mode="train", transform=None):

            assert mode in {"train", "valid", "test"}

            self.root = root
            self.mode = mode
            self.transform = transform

            self.images_directory = os.path.join(self.root, "images")
            self.masks_directory = os.path.join(self.root, "annotations", "trimaps")

            self.filenames = self._read_split()  # read train/valid/test splits

        def __len__(self):
            return len(self.filenames)

        def __getitem__(self, idx):

            filename = self.filenames[idx]
            image_path = os.path.join(self.images_directory, filename.strip('\n') + ".jpg")
            mask_path = os.path.join(self.masks_directory, filename.strip('\n') + ".png")

            image = np.array(Image.open(image_path).convert("RGB"))

            trimap = np.array(Image.open(mask_path))
            mask = self._preprocess_mask(trimap)

            # resize images
            image = np.array(Image.fromarray(image).resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LINEAR))
            mask = np.array(Image.fromarray(mask).resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.NEAREST))
            trimap = np.array(Image.fromarray(trimap).resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.NEAREST))

            # convert to other format HWC -> CHW
            image = np.moveaxis(image, -1, 0)
            mask = np.expand_dims(mask, 0)
            trimap = np.expand_dims(trimap, 0)

            sample = dict(image=image, mask=mask, trimap=trimap)
            if self.transform is not None:
                sample = self.transform(**sample)

            return sample

        @staticmethod
        def _preprocess_mask(mask):
            mask = mask.astype(np.float32)
            mask[mask == 2.0] = 0.0
            mask[(mask == 1.0) | (mask == 3.0)] = 1.0
            return mask

        def _read_split(self):
            if self.mode == 'test':
                return open(os.path.join(self.root, 'annotations', 'test.txt'), mode='r').readlines()
            if self.mode == 'train':
                return open(os.path.join(self.root, 'annotations', 'train.txt'), mode='r').readlines()
            if self.mode == 'valid':
                return open(os.path.join(self.root, 'annotations', 'validate.txt'), mode='r').readlines()

        def extract_archive(filepath):
            extract_dir = os.path.dirname(os.path.abspath(filepath))
            dst_dir = os.path.splitext(filepath)[0]
            if not os.path.exists(dst_dir):
                shutil.unpack_archive(filepath, extract_dir)


    # init train, val, test sets
    root = "/Users/dmitry/PycharmProjects/screen_project/screen_project/input_data/InputData512Linear"
    train_dataset = OxfordPetDataset(root, "train")
    valid_dataset = OxfordPetDataset(root, "valid")
    test_dataset = OxfordPetDataset(root, "test")

    # It is a good practice to check datasets don`t intersects with each other
    assert set(test_dataset.filenames).isdisjoint(set(train_dataset.filenames))
    assert set(test_dataset.filenames).isdisjoint(set(valid_dataset.filenames))
    assert set(train_dataset.filenames).isdisjoint(set(valid_dataset.filenames))

    print(f"Train size: {len(train_dataset)}")
    print(f"Valid size: {len(valid_dataset)}")
    print(f"Test size: {len(test_dataset)}")

    n_cpu = os.cpu_count()
    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=n_cpu)
    valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=n_cpu)
    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=n_cpu)

    # lets look at some samples

    # sample = train_dataset[0]
    # plt.subplot(1,2,1)
    # plt.imshow(sample["image"].transpose(1, 2, 0)) # for visualization we have to transpose back to HWC
    # plt.subplot(1,2,2)
    # plt.imshow(sample["mask"].squeeze())  # for visualization we have to remove 3rd dimension of mask
    # plt.show()

    # sample = valid_dataset[0]
    # plt.subplot(1,2,1)
    # plt.imshow(sample["image"].transpose(1, 2, 0)) # for visualization we have to transpose back to HWC
    # plt.subplot(1,2,2)
    # plt.imshow(sample["mask"].squeeze())  # for visualization we have to remove 3rd dimension of mask
    # plt.show()

    # sample = test_dataset[0]
    # plt.subplot(1,2,1)
    # plt.imshow(sample["image"].transpose(1, 2, 0)) # for visualization we have to transpose back to HWC
    # plt.subplot(1,2,2)
    # plt.imshow(sample["mask"].squeeze())  # for visualization we have to remove 3rd dimension of mask
    # plt.show()

    """## Model"""


    class PetModel(pl.LightningModule):

        def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):
            super().__init__()
            self.model = smp.create_model(
                arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs
            )

            # preprocessing parameteres for image
            params = smp.encoders.get_preprocessing_params(encoder_name)
            self.register_buffer("std", torch.tensor(params["std"]).view(1, 3, 1, 1))
            self.register_buffer("mean", torch.tensor(params["mean"]).view(1, 3, 1, 1))

            # for image segmentation dice loss could be the best first choice
            self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)

        def forward(self, image):
            # normalize image here
            image = (image - self.mean) / self.std
            mask = self.model(image)
            return mask

        def shared_step(self, batch, stage):
            image = batch["image"]

            # Shape of the image should be (batch_size, num_channels, height, width)
            # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]
            assert image.ndim == 4

            # Check that image dimensions are divisible by 32,
            # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of
            # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have
            # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80
            # and we will get an error trying to concat these features
            h, w = image.shape[2:]
            assert h % 32 == 0 and w % 32 == 0

            mask = batch["mask"]

            # Shape of the mask should be [batch_size, num_classes, height, width]
            # for binary segmentation num_classes = 1
            assert mask.ndim == 4

            # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation
            assert mask.max() <= 1.0 and mask.min() >= 0

            logits_mask = self.forward(image)

            # Predicted mask contains logits, and loss_fn param `from_logits` is set to True
            loss = self.loss_fn(logits_mask, mask)

            # Lets compute metrics for some threshold
            # first convert mask values to probabilities, then
            # apply thresholding
            prob_mask = logits_mask.sigmoid()
            pred_mask = (prob_mask > 0.5).float()

            # We will compute IoU metric by two ways
            #   1. dataset-wise
            #   2. image-wise
            # but for now we just compute true positive, false positive, false negative and
            # true negative 'pixels' for each image and class
            # these values will be aggregated in the end of an epoch
            tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode="binary")

            return {
                "loss": loss,
                "tp": tp,
                "fp": fp,
                "fn": fn,
                "tn": tn,
            }

        def shared_epoch_end(self, outputs, stage):
            # aggregate step metics
            tp = torch.cat([x["tp"] for x in outputs])
            fp = torch.cat([x["fp"] for x in outputs])
            fn = torch.cat([x["fn"] for x in outputs])
            tn = torch.cat([x["tn"] for x in outputs])

            # per image IoU means that we first calculate IoU score for each image
            # and then compute mean over these scores
            per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction="micro-imagewise")

            # dataset IoU means that we aggregate intersection and union over whole dataset
            # and then compute IoU score. The difference between dataset_iou and per_image_iou scores
            # in this particular case will not be much, however for dataset
            # with "empty" images (images without target class) a large gap could be observed.
            # Empty images influence a lot on per_image_iou and much less on dataset_iou.
            dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction="micro")

            metrics = {
                f"{stage}_per_image_iou": per_image_iou,
                f"{stage}_dataset_iou": dataset_iou,
            }

            self.log_dict(metrics, prog_bar=True)

        def training_step(self, batch, batch_idx):
            return self.shared_step(batch, "train")

        def training_epoch_end(self, outputs):
            return self.shared_epoch_end(outputs, "train")

        def validation_step(self, batch, batch_idx):
            return self.shared_step(batch, "valid")

        def validation_epoch_end(self, outputs):
            return self.shared_epoch_end(outputs, "valid")

        def test_step(self, batch, batch_idx):
            return self.shared_step(batch, "test")

        def test_epoch_end(self, outputs):
            return self.shared_epoch_end(outputs, "test")

        def configure_optimizers(self):
            return torch.optim.Adam(self.parameters(), lr=0.0001)


    model = PetModel("Unet", "timm-mobilenetv3_large_100", in_channels=3, out_classes=1)

    """## Training"""

    # !nvidia-smi -L

    # trainer = pl.Trainer(
    # accelerator='cpu', devices=1, max_epochs=5, default_root_dir='/content/gdrive/MyDrive/screen_project',
    # resume_from_checkpoint='/content/gdrive/MyDrive/screen_project/lightning_logs/version_0/checkpoints/epoch=4-step=420.ckpt'
    # )
    # trainer.fit(
    #     model,
    #     train_dataloaders=train_dataloader,
    #     val_dataloaders=valid_dataloader,
    # )
    #
    # torch.save(model, 'model.pt')

    trainer = pl.Trainer(
        accelerator='cpu', devices=1, max_epochs=5,
        default_root_dir='/Users/dmitry/PycharmProjects/screen_project/screen_project'
    )

    trainer.fit(
        model,
        train_dataloaders=train_dataloader,
        val_dataloaders=valid_dataloader,
    )

    """## Validation and test metrics"""


    def validate(trainer):
        # run validation dataset
        valid_metrics = trainer.validate(model, dataloaders=valid_dataloader, verbose=False)
        pprint(valid_metrics)

        # run test dataset
        test_metrics = trainer.test(model, dataloaders=test_dataloader, verbose=False)
        pprint(test_metrics)


    """##Convert model"""


    def result():
        """# Result visualization"""

        batch = next(iter(test_dataloader))
        with torch.no_grad():
            model.eval()
            logits = model(batch["image"])
        pr_masks = logits.sigmoid()
        print(len(logits))
        print(len(logits[0]))
        print(len(logits[0][0]))
        print(len(logits[0][0][0]))
        for image, gt_mask, pr_mask in zip(batch["image"], batch["mask"], pr_masks):
            plt.figure(figsize=(10, 5))

            plt.subplot(1, 3, 1)
            plt.imshow(image.numpy().transpose(1, 2, 0))  # convert CHW -> HWC
            plt.title("Image")
            plt.axis("off")

            plt.subplot(1, 3, 2)
            plt.imshow(gt_mask.numpy().squeeze())  # just squeeze classes dim, because we have only one class
            plt.title("Ground truth")
            plt.axis("off")

            plt.subplot(1, 3, 3)
            plt.imshow(pr_mask.numpy().squeeze())  # just squeeze classes dim, because we have only one class
            plt.title("Prediction")
            plt.axis("off")

            plt.show()
